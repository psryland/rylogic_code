# Parameters can be overwritten on the command line
# for example: cntk configFile=myConfigFile command=Output
# For running from Visual Studio add
# currentDirectory=$(SolutionDir)/<path to corresponding data folder> 

# The commands to execute
command = Train:Output:Test

# deviceId=-1 for CPU, >=0 for GPU devices, "auto" chooses the best GPU, or CPU if no usable GPU is available
deviceId = 0 #"auto" #-1

# float accuracy
precision = "double"

# diagnostics
traceLevel = 0

training_data   = "./Data/training.txt"
testing_data    = "./Data/testing.txt"
eval_data       = "./Data/eval.txt"
eval_result     = "./Data/result.txt"
label_mapping   = "./Data/label_mapping.txt"
modelPath       = "./Model/forex.dnn"

feature_dim = 46
labels_dim = 3

# TRAINING CONFIG
Train = {
	action = "train"
	makeMode = false #true #

	BrainScriptNetworkBuilder = {
		fdim0 = $feature_dim$
		ldimn = $labels_dim$

		# Inputs
		features = Input{fdim0}
		labels = Input{ldimn}
	
		# This is a function definition.
		# It defines how the neurons propagate data between each layer using weights, bias', and functions
		model(features) = {
			# Hidden layer dimension
			hdim1 = 30
			hdim2 = 30

			# Network model parameters
			W0 = ParameterTensor {(hdim1:fdim0)} ; b0 = ParameterTensor {(hdim1)}
			W1 = ParameterTensor {(hdim2:hdim1)} ; b1 = ParameterTensor {(hdim2)}
			Wn = ParameterTensor {(ldimn:hdim2)} ; bn = ParameterTensor {(ldimn)}

			# How each layer is combined
			f0 = features
			h1 = (W0 * f0 + b0)
			h2 = (W1 * h1 + b1)
			z  = (Wn * h2 + bn)
		}

		# Call the 'model' function using 'features' as the input
		out = model(features)

		# Define criteria and output(s)
		ce   = CrossEntropyWithSoftmax(labels, out.z)
		errs = ClassificationError(Softmax(out.z), labels)
		res  = Softmax(out.z)

		# connect to the system. These five variables must be named exactly like this.
		featureNodes    = (features)
		labelNodes      = (labels)
		criterionNodes  = (ce)
		evaluationNodes = (errs)
		outputNodes     = (res)
	}

	SGD = {
		epochSize = 0  # epochSize = 0 means epochSize is the size of the training set 
		minibatchSize = 25
		#learningRatesPerMB = 0.5
		#learningRatesPerMB = 0.5:0.2*20:0.1
		learningRatesPerMB = 0.5:0.2*5:0.1*5:0.02*5
		momentumPerMB = 0.9
		dropoutRate = 0.0
		maxEpochs = 50
	}

	reader = {
		readerType = "CNTKTextFormatReader"
		file = $training_data$
		input = {
			features = { dim = $feature_dim$ ; format = "dense" }
			labels   = { dim = $labels_dim$ ; format = "dense" }
		}
	}
}

# TEST/EVAL
#  Computes prediction error and perplexity on a test set and writes the output to the console.
Test = {
	action = "test"
	reader = {
		readerType = "CNTKTextFormatReader"
		file = $testing_data$
		input = {
			features = { dim = $feature_dim$ ; format = "dense" }
			labels   = { dim = $labels_dim$ ; format = "dense" }
		}
	}
}

# OUTPUT RESULTS
#  Computes the labels for a test set and writes the results to a file.
Output = {
	action = "write"
	reader = {
		readerType = "CNTKTextFormatReader"
		file = $testing_data$
		input = {
			features = { dim = $feature_dim$ ; format = "dense" }
			labels = { dim = $labels_dim$ ; format = "dense" }
		}
	}

	# Select the nodes to print. If this is not given, the nodes tagged as "output" are printed instead.
	#outputNodeNames = errs

	outputPath = $eval_result$ # Path to write to. ".NODENAME" will be appended.
	#outputPath = "-"          # As an alternative, this prints to stdout.

	# extra formatting options
	# This is configured to print the classified and ground-truth labels.
	# To write out the actual posterior probabilities, comment out this section.
	format = {
		#type = "category"                  # This finds the highest-scoring entry and prints its index.
		labelMappingFile = $label_mapping$ # Specifying this as well will translate the index into a string.
		#sequenceEpilogue = "\t// %s\n"     # Append this on every line, %s = node name. Useful when printing to stdout.
	
		# How to implement simple error counting with this (cd into $OutputDir$ first):
		#    grep PosteriorProb SimpleOutput.PosteriorProb | awk '{print $1}' > P
		#    grep labels        SimpleOutput.labels        | awk '{print $1}' > L
		#    diff L P | grep "<" | wc -l
		#    wc -l P
		# The ratio of the two numbers gives the same error rate as ClassificationError/Sample in the log.
	}
}


# dump parameter values
DumpNodeInfo = [
    action = "dumpNode"
    printValues = true
]